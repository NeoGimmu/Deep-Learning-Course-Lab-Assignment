# NumPy实现CNN实验报告

## 一、实验目的

本实验旨在使用NumPy从零实现一个卷积神经网络（CNN），用于MNIST手写数字识别任务。通过本实验，深入理解CNN的工作原理，包括卷积层、池化层、全连接层、激活函数和反向传播算法的实现细节。

## 二、实验环境

- **操作系统**: Windows
- **编程语言**: Python 3.x
- **主要库**:
  - NumPy: 用于张量运算和矩阵操作
  - Matplotlib: 用于可视化训练结果和预测
  - PyTorch (torchvision): 用于加载MNIST数据集

## 三、实验方法

### 3.1 网络架构

本实验实现的CNN网络结构如下：

```
输入 (28×28×1) 
  ↓
Conv2D (1→16通道, 3×3卷积核, stride=1, padding=1)
  ↓
ReLU激活函数
  ↓
MaxPool2D (2×2池化窗口, stride=2)
  ↓
Conv2D (16→32通道, 3×3卷积核, stride=1, padding=1)
  ↓
ReLU激活函数
  ↓
MaxPool2D (2×2池化窗口, stride=2)
  ↓
Flatten (展平为1维向量)
  ↓
Dense (1568→128)
  ↓
ReLU激活函数
  ↓
Dense (128→10)
  ↓
Softmax (输出10个类别的概率)
```

### 3.2 核心组件实现

#### 3.2.1 卷积层 (Conv2D)

卷积层实现了标准的2D卷积操作，支持自定义输入通道数、输出通道数、卷积核大小、步长和填充。

**前向传播**:
- 对输入进行padding（如果需要）
- 使用滑动窗口方式计算卷积
- 每个输出位置计算输入区域与卷积核的点积
- 加上偏置项

**反向传播**:
- 计算卷积核梯度：输入与输出梯度的卷积
- 计算偏置梯度：输出梯度的求和
- 计算输入梯度：卷积核与输出梯度的卷积

#### 3.2.2 最大池化层 (MaxPool2D)

最大池化层用于降低特征图的空间维度，减少计算量和参数数量。

**前向传播**:
- 使用滑动窗口在输入上取最大值
- 记录最大值的位置索引，用于反向传播

**反向传播**:
- 只将梯度传递到前向传播中最大值的位置
- 其他位置梯度为0

#### 3.2.3 全连接层 (Dense)

全连接层实现标准的矩阵乘法操作。

**前向传播**: `output = input × weights + bias`

**反向传播**:
- 权重梯度：`dW = input.T × dout`
- 偏置梯度：`db = sum(dout, axis=0)`
- 输入梯度：`dx = dout × weights.T`

#### 3.2.4 激活函数

- **ReLU**: `f(x) = max(0, x)`，反向传播时只传递正值的梯度
- **Softmax**: 将输出转换为概率分布，用于多分类任务

#### 3.2.5 损失函数

使用交叉熵损失函数：

```
Loss = -Σ(y × log(p))
```

其中y是真实标签的one-hot编码，p是模型预测的概率分布。

### 3.3 训练过程

1. **数据加载**: 使用torchvision加载MNIST数据集，转换为NumPy数组格式
2. **数据预处理**: 
   - 将图像从(C, H, W)格式转换为(H, W, C)格式
   - 将标签转换为one-hot编码
3. **前向传播**: 计算模型预测
4. **损失计算**: 计算预测与真实标签的交叉熵损失
5. **反向传播**: 计算各层梯度
6. **权重更新**: 使用梯度下降法更新参数
7. **评估**: 在测试集上评估模型性能

## 四、实验结果

### 4.1 训练参数

- **训练集大小**: 2000张图像
- **测试集大小**: 200张图像
- **训练轮次**: 5 epochs
- **批次大小**: 128
- **学习率**: 0.01

### 4.2 训练过程记录

| Epoch | 训练损失 | 训练准确率 | 测试损失 | 测试准确率 |
|-------|---------|-----------|---------|-----------|
| 1     | 2.3026  | 9.80%     | 2.3023  | 12.00%    |
| 2     | 2.3025  | 11.20%    | 2.3020  | 12.00%    |
| 3     | 2.3023  | 11.20%    | 2.3017  | 12.00%    |
| 4     | 2.3023  | 11.20%    | 2.3014  | 12.00%    |
| 5     | 2.3022  | 11.20%    | 2.3011  | 12.00%    |

### 4.3 可视化结果

实验生成了两个可视化文件：

1. **training_history.png**: 展示了训练和测试的损失、准确率随epoch变化曲线
2. **predictions.png**: 展示了模型在测试集上的预测结果示例

## 五、结果分析

### 5.1 模型性能分析

从实验结果可以看出：

1. **收敛速度**: 模型在训练初期收敛较慢，损失值从2.3026缓慢下降到2.3022
2. **准确率**: 
   - 训练准确率从9.80%提升到11.20%
   - 测试准确率保持在12.00%
3. **过拟合现象**: 训练准确率和测试准确率差距较小，未出现明显过拟合

### 5.2 性能不佳的原因分析

模型准确率较低（约11-12%）的可能原因：

1. **训练数据量不足**: 仅使用2000张训练图像，远少于MNIST完整数据集的60000张
2. **训练轮次过少**: 仅训练5个epoch，模型可能还未充分学习
3. **学习率可能过大**: 学习率为0.01可能导致优化过程不稳定
4. **网络结构相对简单**: 仅使用2个卷积层，可能不足以提取足够的特征
5. **权重初始化**: 使用随机初始化，可能需要更好的初始化策略

### 5.3 与预期结果的对比

MNIST手写数字识别是一个相对简单的任务，使用深度学习框架（如PyTorch、TensorFlow）实现的CNN通常可以达到98%以上的准确率。本实验使用纯NumPy实现，准确率较低的主要原因：

1. **实现效率**: 纯NumPy实现的卷积操作效率较低，限制了训练规模
2. **优化技术**: 缺乏批量归一化、Dropout等正则化技术
3. **优化器**: 仅使用简单的梯度下降，未使用Adam等自适应优化器

## 六、改进建议

基于实验结果，提出以下改进建议：

### 6.1 数据层面

1. **增加训练数据量**: 使用完整的MNIST训练集（60000张图像）
2. **数据增强**: 对训练图像进行旋转、平移、缩放等增强操作
3. **归一化**: 对输入图像进行标准化处理

### 6.2 模型层面

1. **增加网络深度**: 添加更多卷积层，提取更丰富的特征
2. **增加网络宽度**: 增加每层的通道数
3. **添加正则化**: 引入Dropout层防止过拟合
4. **批量归一化**: 在卷积层后添加Batch Normalization

### 6.3 训练策略

1. **调整学习率**: 使用学习率衰减策略
2. **增加训练轮次**: 训练更多epoch（如20-50个）
3. **使用更好的优化器**: 实现Adam或RMSprop优化器
4. **早停机制**: 根据验证集性能提前停止训练

## 七、实验总结

1. 成功使用纯NumPy实现了完整的CNN网络，包括：
   - 卷积层（Conv2D）
   - 最大池化层（MaxPool2D）
   - 全连接层（Dense）
   - 激活函数（ReLU、Softmax）
   - 损失函数（交叉熵）
   - 反向传播算法

2. 实现了完整的训练和评估流程
3. 生成了训练曲线和预测结果的可视化

## 八、附录

### 8.1 文件说明

- `numpy_cnn_mnist.py`: CNN模型实现代码
- `training_history.png`: 训练曲线可视化
- `predictions.png`: 预测结果可视化
- `实验报告.md`: 本实验报告

### 8.2 代码结构

```
numpy_cnn_mnist.py
├── Conv2D类              # 卷积层实现
├── MaxPool2D类           # 最大池化层实现
├── Dense类               # 全连接层实现
├── relu/relu_backward    # ReLU激活函数
├── softmax               # Softmax激活函数
├── CrossEntropyLoss类    # 交叉熵损失函数
├── CNN类                 # CNN模型主类
├── load_mnist_data       # 数据加载函数
├── visualize_predictions # 预测结果可视化
├── plot_training_history # 训练曲线可视化
└── main                  # 主程序入口
